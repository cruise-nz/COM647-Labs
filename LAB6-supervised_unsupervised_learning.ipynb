{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731b9698",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "- Liner Methods (simple, intepretable)\n",
    "    - Linear Regression [Regression]\n",
    "    - Logistic Regression [Classification]\n",
    "- Ensemble Methods (improve accuracy, reduce overfitting)\n",
    "    - Random Forest Regressor [Regression]\n",
    "    - Gradient Boosting Classifier [Classification]\n",
    "- Kernel-Based Methods (high-dimensional boundaries)\n",
    "    - Support Vector Regression [Regression]\n",
    "    - Linear Support Vector Classifier [Classification]\n",
    "- Tree-based (Handle Nonlinearities, Interactions) --> Decision Tree, Gradient Boosting Classifier\n",
    "- Instance-based (memory-based, distance-driven) --> K-Nearest Neighbors\n",
    "- Probabilistic Methods (based on likelihoods) --> Bayesian regression, Naive Bayes\n",
    "- Neural Network & Deep Learnig (complex, flexible, data-hungry) --> MLP regressor, Deep Nets, CNN, RNN\n",
    "- Hyperparameter Tuning\n",
    "    - K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d159702",
   "metadata": {},
   "source": [
    "### Linear Methods\n",
    "- Linear models assume a linear relationship between input features and the output.\n",
    "- For regression: output is a weighted sum of features.\n",
    "- For classification: output is modeled as a probability via functions like the logistic.\n",
    "- They are often the baseline models in machine learning because of their simplicity and interpretability.\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <th>Pros</th>\n",
    "    <th>Cons</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Simple & interpretable → coefficients show direct feature influence.</td>\n",
    "    <td>Linearity assumption → fails if relationships are nonlinear or complex.</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Computationally efficient → fast to train even on large datasets.</td>\n",
    "    <td>Sensitive to multicollinearity & outliers → can distort coefficients.</td>\n",
    "  </tr>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Low data requirement → works well with smaller datasets.</td>\n",
    "    <td>Limited flexibility → cannot easily model feature interactions unless engineered.</td>\n",
    "  </tr>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Regularization available (Ridge/Lasso/Elastic Net) → helps control overfitting and perform feature selection.</td>\n",
    "    <td>Not always the most accurate → often outperformed by ensembles or deep learning on complex data.</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db4343",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d395a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cd1019d",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "- Ensemble methods combine multiple models (often weak learners like decision trees) to produce a stronger, more accurate model..\n",
    "- Bagging (e.g., Random Forest) → trains many models on random subsets of the data and averages their predictions. This reduces variance and makes results more stable.\n",
    "- Boosting (e.g., XGBoost, LightGBM, AdaBoost) → builds models one after another, with each new model fixing the mistakes of the previous ones. This reduces bias and improves accuracy.\n",
    "- Stacking / Voting → combines predictions from different models.\n",
    "    - Voting: takes majority vote (classification) or average (regression).\n",
    "    - Stacking: trains another model (meta-learner) to best combine the outputs.\n",
    "\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <th>Pros</th>\n",
    "    <th>Cons</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>High accuracy → often outperform single models.</td>\n",
    "    <td>Less interpretable → hard to explain decisions compared to simple models.</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Handles nonlinearity & feature interactions well.</td>\n",
    "    <td>Computationally expensive → training and prediction can be slow on large datasets.</td>\n",
    "  </tr>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Robustness → less prone to overfitting than individual trees.</td>\n",
    "    <td>Hyperparameter tuning is critical and can be complex.</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda6875",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp647_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
